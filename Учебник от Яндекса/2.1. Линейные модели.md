## Почему модели линейные?

Задачи классификации и регрессии можно сформулировать как поиск отображения из множества объектов $\mathbb{X}$ в множество возможных таргетов. Математически это можно описать так:

- **Классификация**: $\mathbb{X} \rightarrow 0, 1, ..., K$, где $0, ..., K$ - номера классов
- **Регрессия**: $\mathbb{X} \rightarrow \mathbb{R}$

Мы бы хотели найти такое отображение, которое лучше всего приближает истинное соответствие между объектами и таргетами. Что такое "лучше всего" - сложный вопрос, на который нет однозначного ответа. Но можно для начала определить, среди каких отображений мы будем искать самое лучшее. Возможных отображений может быть много, но мы можем упростить себе задачу и договориться, что хотим искать решение только в каком-то заранее заданном параметризированном семействе функций. Этот параграф будет посвящен самому простому такому семейству — линейным функциям вида:

$y = \omega_0 + \omega_1 x_1 + ... + \omega_D x_D$,

где $y$ - целевая переменная / **таргет**, (x_1, ..., x_D) - вектор, соответствующий объекту выборки / **вектор признаков**, а $\omega_0, \omega_1, ..., \omega_D$ - параметры модели. Признаки ещё называют **фичами** (от английского **features**). Вектор $\omega = (\omega_1, ..., \omega_D)$ часто называют вектором весов, а число $\omega_0$ - свободным коэффициентом или **сдвигом** (bias). Более компактно линейную модель можно записать в виде:

$y = \left\langle {x, \omega} \right\rangle + \omega_0$

**Замечание:** Чтобы применять линейную модель, нужно, чтобы каждый объект уже был представлен вектором численных признаков $x_1, ..., x_D$.

