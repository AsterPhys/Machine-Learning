## Почему модели линейные?

Задачи классификации и регрессии можно сформулировать как поиск отображения из множества объектов $\mathbb{X}$ в множество возможных таргетов. Математически это можно описать так:

- **Классификация**: $\mathbb{X} \rightarrow 0, 1, ..., K$, где $0, ..., K$ - номера классов
- **Регрессия**: $\mathbb{X} \rightarrow \mathbb{R}$

Мы бы хотели найти такое отображение, которое лучше всего приближает истинное соответствие между объектами и таргетами. Что такое "лучше всего" - сложный вопрос, на который нет однозначного ответа. Но можно для начала определить, среди каких отображений мы будем искать самое лучшее. Возможных отображений может быть много, но мы можем упростить себе задачу и договориться, что хотим искать решение только в каком-то заранее заданном параметризированном семействе функций. Этот параграф будет посвящен самому простому такому семейству — линейным функциям вида:

$y = \omega_0 + \omega_1 x_1 + ... + \omega_D x_D$,

где $y$ - целевая переменная / **таргет**, (x_1, ..., x_D) - вектор, соответствующий объекту выборки / **вектор признаков**, а $\omega_0, \omega_1, ..., \omega_D$ - параметры модели. Признаки ещё называют **фичами** (от английского **features**). Вектор $\omega = (\omega_1, ..., \omega_D)$ часто называют вектором весов, а число $\omega_0$ - свободным коэффициентом или **сдвигом** (bias). Более компактно линейную модель можно записать в виде:

$y = \left\langle {x, \omega} \right\rangle + \omega_0$

**Замечание:** Чтобы применять линейную модель, нужно, чтобы каждый объект уже был представлен вектором численных признаков $x_1, ..., x_D$.

Разберёмся, как будет работать такая модель в случае, если $D = 1$. То есть у наших объектов есть ровно один численный признак, по которому они отличаются. Теперь наша линейная модель будет выглядеть совсем просто: $y = \omega_1 x_1 + \omega_0$. Для задачи регрессии мы теперь пытаемся приблизить значение $y$ какой-то линейной функцией от переменной $x$. А что будет значить линейность для задачи классификации? Давайте вспомним про пример с поиском мошеннических транзакций по картам. Допустим, нам известна ровно одна численная переменная — объём транзакции. Для бинарной классификации транзакций на законные и потенциально мошеннические мы будем искать так называемое **разделяющее правило**: там, где значение функции положительно, мы будем предсказывать один класс, где отрицательно – другой. В нашем примере простейшим правилом будет какое-то пороговое значение объёма транзакций, после которого есть смысл пометить транзакцию как подозрительную. В случае более высоких размерностей вместо прямой будет гиперплоскость с аналогичным смыслом.

Помимо простоты, у линейных моделей есть несколько других достоинств. К примеру, мы можем достаточно легко судить, как влияют на результат те или иные признаки. Скажем, если вес $\omega_i$ положителен, то с ростом i-го признака таргет в случае регрессии будет увеличиваться, а в случае классификации наш выбор будет сдвигаться в пользу одного из классов. Значение весов оже имеет прозрачную интерпретацию: чем вес $\omega_i$ больше, тем «важнее» i-й признак для итогового предсказания. Это качество моделей называют **интерпретируемостью**.

В то же время не всегда стоит доверять весам линейных моделей:

- Если были введены дополнительные более сложные признаки (поиск таких дополнительных признаков называется **feature engineering**), то осмысленность интерпретации будет сильно зависеть от здравого смысла эксперта, строившего модель;
- Если между признаками есть приближённая линейная зависимость, коэффициенты в линейной модели могут совершенно потерять физический смысл;
- Особенно осторожно стоит верить в утверждения вида «этот коэффициент маленький, значит, этот признак не важен». Во-первых, всё зависит от масштаба признака: вдруг коэффициент мал, чтобы скомпенсировать его. Во-вторых, зависимость действительно может быть слабой, но кто знает, в какой ситуации она окажется важна. Такие решения принимаются на основе данных, например, путём проверки статистического критерия;
- Конкретные значения весов могут меняться в зависимости от обучающей выборки, хотя с ростом её размера они будут потихоньку сходиться к весам «наилучшей» линейной модели, которую можно было бы построить по генеральной совокупности.

## Линейная регрессия и метод наименьших квадратов (МНК)

Мы начнём с использования линейных моделей для решения задачи регрессии. Простейшим примером постановки задачи линейной регрессии является **метод наименьших квадратов**.

...
